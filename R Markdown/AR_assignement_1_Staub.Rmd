---
title: "AR_assignment_1"
author: "Alexander Staub"
date: "July 3, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

What is the observed mean difference in the outcome variable Y between the presence and absence of the treatment? Show this mean difference using both a t-test as well as a regression specification. Corroborate, for example in Excel, that this observed difference equals the ratio of the covariance between Y and D over the variance of D.

### Answer 1

First Load the data
```{r}
library(readxl)

df_raw <- read_excel("C:/R work/applied econometrics/applied_econometrics/Data/Session1data.xlsx", 
                     sheet = 1)

```

show the mean difference between presence and absence of treatment in the outcome variable
```{r}
library(stats)
#regression specification
summary(lm(data = df_raw, y ~ D))[[4]]
#T-test
t.test(df_raw$y~df_raw$D)
#ratio covariance to variance
cov(df_raw$y, y = df_raw$D)/var(df_raw$D)

```

from the output above it is clear that the coefficient on the regression does in fact resemble the covariance of treatment and outcome divided by the variance of the treatment

## Question 2

What is the difference in the outcome variable Y between the presence and absence of the treatment after controlling for observed firm characteristics (X1-X4)?
i. Using a new regression, corroborate that the regression coefficient on D in the previous regression equals the ratio of the covariance between Y and that part of D that is orthogonal to X1-X4 over the variance of that part of D that is orthogonal to X1-X4.
ii. What happens to the coefficients on X1-X4?
iii. What happens to the regression coefficients when regressing Y on D and X1-X4 after making all independent variables orthogonal to each other?
iv. At the end of the day, which regression specification should we rely on?

### Answer 2

First I want to calculate the new regression specification

```{r}
#with treatment variable D
summary(lm(data = df_raw, y ~ x1 + x2 + x3 + x4 + D))
#without treatment variable D
summary(lm(data = df_raw, y ~ x1 + x2 + x3 + x4 ))
```

(i) In order to receive the part of d that is orthogonal to the other regressors, I need to regress the treatment D on the controls x1 - x4 and save the residuals as a new variable

```{r}
#regress D on the control variables
df_raw$D_orth <- residuals(lm(D~x1 + x2 + x3 + x4, data = df_raw))
#calculate the ratio of covariance between y and orthogonal d over the variance of orthogonal D
cov(df_raw$D_orth, y = df_raw$y)/var(df_raw$D_orth)

```
as seen above, the regression coefficient of D using the new specification equals the ratio of covariance(y, D orthogonal to X's) and variance(D orthogonal to X's)

(ii) as seen in the output of the regression summary above, the coefficient on x1, x2 and x3 increases, while it decreases on x4. Standard errors on all 4 control variables hardly see any change and thus significance values of the coefficients don't change either

(iii) the process above will be repeated with all the x variables to make them orthogonal to eachother
```{r}
#create orthogonal variables
df_raw$x1_orth <- residuals(lm(x1~D + x2 + x3 + x4, data = df_raw))
df_raw$x2_orth <- residuals(lm(x2~D + x1 + x3 + x4, data = df_raw))
df_raw$x3_orth <- residuals(lm(x3~D + x2 + x1 + x4, data = df_raw))
df_raw$x4_orth <- residuals(lm(x4~D + x2 + x3 + x1, data = df_raw))
#run regression with orthogonal variables
summary(lm(data = df_raw, y~x1_orth + x2_orth + x3_orth + x4_orth + D_orth))
```
after making each variable orthogonal to the remaining variables in the regression, any correlation among them is partialled out, making a multivariate regression unnecessary. All that changed is that the intercept has increased, making the effect size of the variables lower in the multivariate regression with orthogonal variables compared to non-orthogonal variable regression. 

(iv) It would make more sense to use the first specification (i.e. not including orthogonal variables) rather than the second regression as in the second regression, interpretation of the coefficients is no longer straight forward. I.e. a change in the treatment variable from 0 to 1 gives the change in the outcome variable (keeping control variables constant), while the coefficient in the orthogonal treatment variable controlling for orthogonal control variables does not yield the same interpretation. 

## Question 3

 What is the difference in the outcome variable Y between the presence and absence of the treatment after controlling for observed firm characteristics (X1-X4) as well as industry effects?
 
### Answer 3

```{r}
#regression with treatment and industry effects
summary(lm(data = df_raw, y~ x1 + x2 + x3 + x4 + as.factor(industry) + D))[[4]]

```

the outcome variable changes by 0.094 after treatment with industry effects and observed firm characteristics, however remains negative. Including industry effects one can see that hardly any have a significant impact on the outcome variable (apart for industry 6)

## Question 4

Given that the assumption of homoskedasticity is probably not valid and the data are not independent across firms because of industry affiliation, which standard errors should we rely on? The default OLS standard errors (“conventional”), robust standard errors, clustered standard errors, or some other standard errors? Using the regression specification of question 3, determine the significance of the coefficient on D using the standard error that you think is most appropriate and explain why you think it is most appropriate.

### Answer 4

a brief visual investigation when plotting the outcome variable and the x1 variable with and without industry factors shows 2 things. 
-  1st, that variance increases as x1 increases (heteroskedasticity)
-  2nd, that variance is differs for each industry cluster

```{r df_raw, echo=FALSE}
#check for heteroskedasticity
library(ggplot2)
library(gridExtra)

g_1 <- ggplot(data = df_raw, aes(y=y, x=x1)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  ggtitle("outcome on x1 without industry factors")

g_2 <- ggplot(data = df_raw, aes(y=y, x=x1, colour=as.factor(industry))) +
  geom_point(aes(colour=as.factor(industry))) +
  geom_smooth(method = "lm")+
  ggtitle("outcome on x1 with industry factors")

grid.arrange(g_1, g_2)


```

below, the regression coefficients including robust standard errors and clustered standard errors on the industry level

```{r}
library(lmtest)
library(sandwich)
if (!require(miceadds)) install.packages("miceadds"); library(miceadds)  

#compute the regression specification from question 3 with robust standard errors
m_1 <- lm(data = df_raw, y~ x1 + x2 + x3 + x4 + as.factor(industry) + D)
coeftest(m_1, vcov = vcovHC(m_1, type = "HC1"))

#compute with clustered standard errors
m_2 <- lm.cluster(data = df_raw, y~ x1 + x2 + x3 + x4 + as.factor(industry) + D, cluster = "industry")

summary(m_2)
```

above results show that standard errors on the treatment variable double when including clustered standard errors vs robust standard errors. The former is the most plausible specification, given the firms industry affiliation related non-independence. 